{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44543fcf",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8530466",
   "metadata": {},
   "source": [
    "Min-Max scaling is a common data normalization technique used in data preprocessing to transform numerical data features into a specific range. It is also known as feature scaling or data scaling. In Min-Max scaling, the values of a numerical feature are scaled down to a range between 0 and 1. This scaling process is accomplished by subtracting the minimum value of the feature from each data point and then dividing the result by the range of the feature, which is the difference between the maximum and minimum values.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "scaled_value = (value - min) / (max - min)\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset of student test scores, where the score of each student ranges from 0 to 100. We want to normalize this dataset using Min-Max scaling. The minimum and maximum scores in the dataset are 40 and 90, respectively.\n",
    "\n",
    "To apply Min-Max scaling, we subtract the minimum value (40) from each score and divide the result by the range (90 - 40 = 50) of the feature. This gives us the scaled score for each student.\n",
    "\n",
    "For example, if a student's score is 75, the scaled score would be:\n",
    "\n",
    "scaled_score = (75 - 40) / (90 - 40) = 0.625\n",
    "\n",
    "Therefore, the normalized score for this student would be 0.625, which is on the scale between 0 and 1. This process is repeated for each student score in the dataset, and the resulting normalized dataset will have values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca67b75a",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ee32e",
   "metadata": {},
   "source": [
    "The unit vector technique, also known as normalization, is a feature scaling method used to transform the features in a dataset so that they have a unit norm. This technique scales each feature such that its magnitude is 1, and it preserves the direction of the original vector.\n",
    "\n",
    "On the other hand, Min-Max scaling, also known as normalization, scales the features to a specific range, typically between 0 and 1.\n",
    "\n",
    "The main difference between the two techniques is that Min-Max scaling scales the features to a fixed range, whereas the unit vector technique scales each feature to a magnitude of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cddedfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.07106781e+01 1.04880885e+05]\n",
      "-------------------\n",
      "[[0.42426407 0.47673129]\n",
      " [0.56568542 0.57207755]\n",
      " [0.70710678 0.66742381]]\n",
      "------------------\n",
      "[[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([[30, 50000], [40, 60000], [50, 70000]])\n",
    "\n",
    "norms = np.linalg.norm(data, axis=0)\n",
    "print(norms)\n",
    "print(\"-------------------\")\n",
    "unit_data = data / norms\n",
    "print(unit_data)\n",
    "print(\"------------------\")\n",
    "min_vals = data.min(axis=0)\n",
    "max_vals = data.max(axis=0)\n",
    "\n",
    "scaled_data = (data - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062bc929",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f70e4",
   "metadata": {},
   "source": [
    "PCA, or Principal Component Analysis, is a statistical technique used to reduce the number of variables in a dataset while retaining as much information as possible. It accomplishes this by identifying the underlying structure of the data and representing it in a smaller number of dimensions, called principal components.\n",
    "\n",
    "PCA works by finding the directions in the data that have the highest variance, which are also the directions that contain the most information. These directions are called the principal components, and they are orthogonal to each other. By projecting the data onto these principal components, we can reduce the dimensionality of the data while still retaining as much of the original information as possible.\n",
    "\n",
    "To illustrate the application of PCA in dimensionality reduction, let's consider an example. Suppose we have a dataset that contains information on the physical characteristics of different types of fruits, including their weight, diameter, and height. We want to reduce the dimensionality of this dataset to two dimensions so that we can easily visualize the data.\n",
    "\n",
    "We start by applying PCA to the dataset, which identifies the two principal components that capture the most variation in the data. We can then project the data onto these two principal components, creating a new 2D dataset that still contains most of the information from the original dataset.\n",
    "\n",
    "We can then plot this new 2D dataset, which shows us how the different types of fruits are distributed based on their physical characteristics. This visualization can help us identify patterns and relationships in the data that were not immediately apparent in the original high-dimensional dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe7b65",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c9d7d",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts. Feature extraction refers to the process of identifying and selecting the most important features from a dataset to use in a machine learning model. PCA can be used as a feature extraction technique to identify the most important features or dimensions in a dataset.\n",
    "\n",
    "To illustrate this concept, let's consider an example where we have a dataset containing information on different types of flowers, including their petal length, petal width, sepal length, and sepal width. We want to build a machine learning model to classify the flowers into different species based on their physical characteristics.\n",
    "\n",
    "Before building the model, we need to identify which features are the most important for classification. We can use PCA to extract the most important features from the dataset. PCA will identify the principal components that capture the most variation in the data, which will correspond to the most important features.\n",
    "\n",
    "After applying PCA, we can select the top principal components as our features for the machine learning model. These principal components are linear combinations of the original features and represent the most important information in the data.\n",
    "\n",
    "For example, if PCA identifies that the first two principal components capture most of the variation in the data, we can use these two components as our features for the classification model. This reduces the dimensionality of the dataset and simplifies the model, while still retaining most of the important information in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc47d91",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee2650",
   "metadata": {},
   "source": [
    "In the case of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the features such as price, rating, and delivery time. This can help to bring the features onto a common scale, making it easier to compare and analyze them.\n",
    "\n",
    "For example, let's say we have a feature 'price' that ranges from $5 to $50 and a feature 'rating' that ranges from 1 to 5. By using Min-Max scaling, we can rescale these features to a range of 0 to 1. If we set the minimum and maximum values for the 'price' feature as $5 and $50 respectively, and for the 'rating' feature as 1 and 5 respectively, then we can use the formula to rescale the values:\n",
    "\n",
    "price_norm = (price - 5) / (50 - 5)\n",
    "rating_norm = (rating - 1) / (5 - 1)\n",
    "\n",
    "This will give us normalized values for both the 'price' and 'rating' features, which can be used in our recommendation system.\n",
    "\n",
    "Similarly, we can use Min-Max scaling to preprocess the 'delivery time' feature, which might have a range of values from 10 minutes to 60 minutes. By rescaling this feature to a range of 0 to 1, we can make it easier to compare with other features and use it in our recommendation system.\n",
    "\n",
    "In conclusion, Min-Max scaling is a useful technique for preprocessing features in a food delivery recommendation system. By rescaling the features to a common range, we can make it easier to compare and analyze them, which can ultimately help us make more accurate recommendations to users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a47bc0",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72395613",
   "metadata": {},
   "source": [
    "In the context of predicting stock prices, you could use PCA to reduce the dimensionality of the dataset by following these steps:\n",
    "\n",
    "1. Normalize the data: Before applying PCA, it's important to standardize the features so that they all have the same scale. This is because PCA is sensitive to differences in scale between features.\n",
    "\n",
    "2. Calculate the covariance matrix: Next, calculate the covariance matrix of the standardized data. The covariance matrix represents the relationships between the features in the dataset.\n",
    "\n",
    "3. Calculate the eigenvectors and eigenvalues: The eigenvectors and eigenvalues of the covariance matrix can be calculated using a technique such as singular value decomposition (SVD). The eigenvectors represent the directions in the feature space that explain the most variance in the data, and the eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "4. Select the number of principal components: Choose the number of principal components to keep based on the amount of variance they explain. A common approach is to keep enough principal components to explain at least 80% of the variance in the data.\n",
    "\n",
    "5. Project the data onto the new feature space: Finally, project the standardized data onto the new feature space defined by the selected principal components. This will result in a lower-dimensional representation of the original data that can be used for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f0bba",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164b45b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.21052631578947367\n",
      "0.47368421052631576\n",
      "0.7368421052631579\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.array([1, 5, 10, 15, 20])\n",
    "mean=np.mean(a)\n",
    "std=np.std(a)\n",
    "minimum=a.min()\n",
    "maxi=a.max()\n",
    "for i in a:\n",
    "    s=(i-minimum)/(maxi-minimum)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4106767f",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da18262",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to reduce the number of features in a dataset by identifying the most important features or combinations of features. In this case, we have a dataset with five features: height, weight, age, gender, and blood pressure.\n",
    "\n",
    "To determine the number of principal components to retain, we can perform the following steps:\n",
    "\n",
    "Standardize the data: PCA is sensitive to the scale of the features, so we need to standardize the data to have zero mean and unit variance.\n",
    "Compute the covariance matrix: PCA seeks to identify the directions of maximum variance in the data, so we need to compute the covariance matrix of the standardized data.\n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix: The eigenvectors represent the principal components of the data, and the eigenvalues represent the amount of variance explained by each principal component.\n",
    "Choose the number of principal components: We can use a scree plot or cumulative explained variance plot to visualize the amount of variance explained by each principal component and choose the number of components to retain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
