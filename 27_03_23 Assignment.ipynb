{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ac3a67",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e362efcc",
   "metadata": {},
   "source": [
    "R-squared is a statistical measure that represents the proportion of variance in the dependent variable that is explained by the independent variable(s) in a linear regression model. It is also known as the coefficient of determination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e71bd",
   "metadata": {},
   "source": [
    "R-squared is calculated by dividing the sum of the squared differences between the predicted values and the actual values by the total sum of the squared differences between the actual values and the mean of the dependent variable. This calculation produces a value between 0 and 1, with 1 indicating a perfect fit of the model to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df261c",
   "metadata": {},
   "source": [
    "The formula for calculating R-squared is as follows:\n",
    "\n",
    "R-squared = 1 - (SSres / SStot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a7317",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ef403",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of R-squared that takes into account the number of independent variables in a linear regression model. It is designed to provide a more accurate assessment of the model's goodness of fit, particularly when comparing models with different numbers of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa00d61",
   "metadata": {},
   "source": [
    "The main difference between adjusted R-squared and regular R-squared is that adjusted R-squared penalizes the inclusion of additional independent variables that do not significantly contribute to the model's predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4ecf6",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bafec7",
   "metadata": {},
   "source": [
    "It is generally more appropriate to use adjusted R-squared when comparing multiple linear regression models that have different numbers of independent variables. This is because the regular R-squared value tends to increase with the addition of more independent variables, even if those variables do not contribute significantly to the model's predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f51471",
   "metadata": {},
   "source": [
    "Adjusted R-squared takes into account the number of independent variables in the model and adjusts the R-squared value accordingly. This means that it penalizes the inclusion of additional independent variables that do not contribute significantly to the model's predictive power. Therefore, adjusted R-squared is more appropriate for model comparison and selection when the models have different numbers of independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34840fc3",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c57719",
   "metadata": {},
   "source": [
    "###### Root Mean Squared Error (RMSE):\n",
    "RMSE measures the average distance between the predicted and actual values of the dependent variable. It is calculated as the square root of the average of the squared differences between the predicted and actual values.\n",
    "RMSE = sqrt(1/n * Σ(y - ŷ)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992ea83",
   "metadata": {},
   "source": [
    "###### Mean Squared Error (MSE):\n",
    "MSE is the average of the squared differences between the predicted and actual values of the dependent variable.\n",
    "MSE = 1/n * Σ(y - ŷ)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af9734",
   "metadata": {},
   "source": [
    "###### Mean Absolute Error (MAE):\n",
    "MAE measures the average absolute difference between the predicted and actual values of the dependent variable.\n",
    "MAE = 1/n * Σ|y - ŷ|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c583be1",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d4319",
   "metadata": {},
   "source": [
    "##### Advantages of RMSE:\n",
    "\n",
    "RMSE is sensitive to the magnitude of errors, as it takes the square root of the squared errors. This means that large errors have a greater impact on the RMSE value than small errors.\n",
    "RMSE is widely used in machine learning and statistical analysis, and it is often reported alongside other metrics such as R-squared and adjusted R-squared.\n",
    "RMSE is useful when the residuals are normally distributed and the errors have a constant variance, as these assumptions are required for many statistical tests and model selection criteria.\n",
    "##### Disadvantages of RMSE:\n",
    "\n",
    "RMSE can be heavily influenced by outliers, as large errors have a greater impact on the RMSE value than small errors. This can result in an overemphasis on the performance of the model for extreme values, which may not be relevant in practical applications.\n",
    "RMSE can be difficult to interpret, as it is expressed in the same units as the dependent variable.\n",
    "##### Advantages of MSE:\n",
    "\n",
    "MSE is widely used in statistical tests and model selection criteria, as it is an unbiased estimator of the variance of the errors or residuals.\n",
    "MSE is useful when the residuals are normally distributed and the errors have a constant variance, as these assumptions are required for many statistical tests and model selection criteria.\n",
    "##### Disadvantages of MSE:\n",
    "\n",
    "MSE is heavily influenced by outliers, as large errors have a greater impact on the MSE value than small errors. This can result in an overemphasis on the performance of the model for extreme values, which may not be relevant in practical applications.\n",
    "MSE can be difficult to interpret, as it is expressed in the squared units of the dependent variable.\n",
    "##### Advantages of MAE:\n",
    "\n",
    "MAE is more robust to outliers compared to RMSE and MSE, as it only considers the absolute errors and does not give more weight to larger errors.\n",
    "MAE is easy to interpret, as it is expressed in the same units as the dependent variable.\n",
    "##### Disadvantages of MAE:\n",
    "\n",
    "MAE does not take into account the magnitude of errors, as it only considers the absolute errors. This means that large errors and small errors have the same impact on the MAE value.\n",
    "MAE is less commonly used in statistical tests and model selection criteria compared to RMSE and MSE, which may make it more difficult to compare different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dedd458",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9a3ee",
   "metadata": {},
   "source": [
    "Lasso regularization is a type of regularization technique used in linear regression models to prevent overfitting and improve model performance. It works by adding a penalty term to the objective function of the regression model, which is based on the absolute values of the coefficients. The goal of Lasso regularization is to shrink some of the coefficients to zero, effectively performing feature selection and reducing the complexity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0362dec9",
   "metadata": {},
   "source": [
    "The Lasso regularization is similar to Ridge regularization, but with a key difference in the type of penalty term used. Ridge regularization adds a penalty term based on the squared values of the coefficients, whereas Lasso regularization adds a penalty term based on the absolute values of the coefficients. This difference in penalty terms leads to different behavior in terms of the coefficients that are selected and shrunk to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa4f9d",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9687a1b",
   "metadata": {},
   "source": [
    "Regularized linear models, such as Ridge regression and Lasso regression, help to prevent overfitting in machine learning by adding a penalty term to the objective function of the regression model. This penalty term encourages the model to have smaller coefficient values, effectively reducing the complexity of the model and preventing it from fitting the noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3c831",
   "metadata": {},
   "source": [
    "To prevent overfitting, we can use Ridge or Lasso regression, which add a penalty term to the objective function of the linear regression model. The penalty term encourages the model to have smaller coefficient values, effectively reducing the complexity of the model and preventing it from fitting the noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638264b5",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea583dc",
   "metadata": {},
   "source": [
    "Loss of interpretability\n",
    "\n",
    "Sensitivity to hyperparameters\n",
    "\n",
    "Limited handling of non-linear relationships\n",
    "\n",
    "Data limitations\n",
    "\n",
    "Handling of outliers\n",
    "\n",
    "Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ce9c2",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afd796",
   "metadata": {},
   "source": [
    "Deciding which model is better between Model A and Model B depends on the specific requirements and goals of the problem at hand.\n",
    "\n",
    "If the goal is to minimize the magnitude of errors, then Model B with an MAE of 8 is the better performer. This means that, on average, the absolute difference between the predicted and actual values is 8 units, which is smaller than the RMSE of 10 for Model A.\n",
    "\n",
    "However, if the goal is to minimize the impact of large errors, then Model A with an RMSE of 10 may be the better choice. RMSE penalizes large errors more heavily than MAE, as it squares the errors before averaging them. Therefore, Model A may be more suitable for problems where large errors have significant consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9860252",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870638c7",
   "metadata": {},
   "source": [
    "Ridge regularization with a regularization parameter of 0.1 tends to perform better when there are many features with small to moderate effect sizes. It shrinks the coefficients towards zero, but they never become exactly zero. This means that Ridge regression can still include all features in the model, but with smaller coefficients, thereby reducing overfitting. Lasso regularization with a regularization parameter of 0.5, on the other hand, tends to perform better when there are many features but only a few of them have a large effect size. Lasso regression can set some coefficients to exactly zero, effectively performing feature selection and reducing the complexity of the model.\n",
    "\n",
    "Therefore, the choice between Ridge and Lasso regularization depends on the specific characteristics of the data and the goals of the problem at hand. If the goal is to include all features in the model and reduce overfitting, Ridge regularization may be a better choice. If the goal is to perform feature selection and reduce the complexity of the model, Lasso regularization may be more appropriate.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
