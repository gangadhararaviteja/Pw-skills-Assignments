{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c2fcb0",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5469ea0e",
   "metadata": {},
   "source": [
    "Missing values in a dataset are values that are absent or undefined for one or more variables in the dataset. These values can occur due to a variety of reasons such as data entry errors, data loss during transmission, or non-response by participants in a survey. Missing values can be denoted by symbols such as \"NA,\" \"N/A,\" \"null,\" \"NaN,\" or simply left blank.\n",
    "\n",
    "It is essential to handle missing values because they can lead to biased or inaccurate analysis, and can also affect the performance of machine learning models. Some of the common reasons why missing values need to be handled are:\n",
    "\n",
    "- They can lead to biased estimates of statistical parameters such as means, variances, and correlations.\n",
    "\n",
    "- They can reduce the sample size and statistical power of analyses, leading to reduced accuracy and generalizability.\n",
    "\n",
    "- They can introduce errors or bias into predictive models, resulting in poor model performance.\n",
    "\n",
    "- They can result in missing or incorrect data in downstream analyses, leading to inaccurate conclusions.\n",
    "\n",
    "Some algorithms that are not affected by missing values include:\n",
    "\n",
    "1) Decision Trees: Decision trees can handle missing values by considering the most common value or mean of the feature in the training data.\n",
    "\n",
    "2) Random Forest: Random forest can handle missing values by considering the most common value or mean of the feature in the training data or by using surrogate splits.\n",
    "\n",
    "3) Gradient Boosting: Gradient boosting can handle missing values by considering the most common value or mean of the feature in the training data or by using surrogate splits.\n",
    "\n",
    "4) K-Nearest Neighbors (KNN): KNN can handle missing values by imputing the missing values with the mean or median value of the feature in the training data.\n",
    "\n",
    "5) Support Vector Machines (SVM): SVM can handle missing values by considering the most common value or mean of the feature in the training data or by using the value zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3ac69",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e47c2",
   "metadata": {},
   "source": [
    "There are several techniques to handle missing data in a dataset. Some commonly used techniques are:\n",
    "\n",
    "1. Deletion: This technique involves removing the rows or columns with missing values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fee255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [A, B, C]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
    "                   'B': [5, None, 7, 8],\n",
    "                   'C': [None, 10, 11, None]})\n",
    "df1 = df.dropna()\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b612f33",
   "metadata": {},
   "source": [
    "Imputation: This technique involves filling in the missing values with an estimated or calculated value. There are several ways to impute missing values:\n",
    "\n",
    "a. Mean/Median imputation: It replaces the missing values with the mean/median value of that feature.\n",
    "\n",
    "b. Mode imputation: It replaces the missing values with the most common value of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c46f203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B     C\n",
      "0  1.000000  5.000000  10.5\n",
      "1  2.000000  6.666667  10.0\n",
      "2  2.333333  7.000000  11.0\n",
      "3  4.000000  8.000000  10.5\n",
      "---------------------------\n",
      "     A    B     C\n",
      "0  1.0  5.0  10.5\n",
      "1  2.0  7.0  10.0\n",
      "2  2.0  7.0  11.0\n",
      "3  4.0  8.0  10.5\n",
      "---------------------------\n",
      "     A    B     C\n",
      "0  1.0  5.0  10.0\n",
      "1  2.0  7.0  10.0\n",
      "2  4.0  7.0  11.0\n",
      "3  4.0  8.0   NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
    "                   'B': [5, None, 7, 8],\n",
    "                   'C': [None, 10, 11, None]})\n",
    "\n",
    "# Mean imputation\n",
    "df1 = df.fillna(df.mean())\n",
    "print(df1)\n",
    "print(\"---------------------------\")\n",
    "#Median imputation\n",
    "df2=df.fillna(df.median())\n",
    "print(df2)\n",
    "print(\"---------------------------\")\n",
    "#Mode imputation\n",
    "df3=df.fillna(df.mode())\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e3725",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6c51e",
   "metadata": {},
   "source": [
    " Imbalanced data refers to a dataset where the distribution of classes or targets is not uniform. In other words, some classes or targets may have significantly more or fewer samples than others. For example, in a binary classification problem, if there are 90% samples of class A and only 10% samples of class B, then it is an imbalanced dataset.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased models that have a high accuracy for the majority class but perform poorly for the minority class. In other words, the model will be more likely to predict the majority class, which may not be desirable in some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3eaa9",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d5d36",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are techniques used to handle imbalanced data by changing the distribution of the classes.\n",
    "\n",
    "Upsampling involves increasing the number of samples in the minority class by adding more copies of the existing samples. This is done until the number of samples in the minority class is equal to or close to the number of samples in the majority class\n",
    "\n",
    "Downsampling involves reducing the number of samples in the majority class by randomly removing samples from that class. This is done until the number of samples in the majority class is equal to or close to the number of samples in the minority class.\n",
    "\n",
    "Suppose we have a dataset with two classes, A and B, where class A has 90 samples and class B has 10 samples. This is an imbalanced dataset, and we need to balance the classes.\n",
    "\n",
    "If we upsample class B using SMOTE, we can generate synthetic samples of class B by interpolating the features of the existing samples. This will increase the number of samples in class B, making it equal to or close to the number of samples in class A.\n",
    "\n",
    "On the other hand, if we downsample class A using Random Under-Sampling, we can randomly remove some samples from class A to reduce its size. This will make the number of samples in class A equal to or close to the number of samples in class B.\n",
    "\n",
    "In general, we upsample the minority class when we have a small number of samples in that class, and downsample the majority class when we have a large number of samples in that class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb7e9e",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a07d9",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by generating new samples based on the existing data. The aim of data augmentation is to improve the performance of a machine learning model by providing it with more training data.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to handle imbalanced datasets. SMOTE generates synthetic samples of the minority class by interpolating the features of existing samples.\n",
    "\n",
    "The SMOTE algorithm works as follows:\n",
    "\n",
    "For each sample in the minority class, it identifies its k nearest neighbors in the feature space.\n",
    "It selects one of the k neighbors at random and computes the difference between the features of the sample and the selected neighbor.\n",
    "It multiplies this difference by a random number between 0 and 1 and adds it to the features of the sample to create a new synthetic sample.\n",
    "This process is repeated until the desired number of synthetic samples is generated.\n",
    "The new synthetic samples are created within the feature space defined by the minority class, thus making them more representative of the minority class. SMOTE can generate many synthetic samples from a single sample, making it an efficient way to upsample the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a65468",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27ccb2",
   "metadata": {},
   "source": [
    "Outliers are observations in a dataset that significantly deviate from other observations. In other words, outliers are data points that are unusually different from the rest of the data. Outliers can be caused by measurement errors, data entry errors, or genuine rare events.\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on the statistical properties of the dataset and the machine learning models trained on the dataset. Outliers can skew the distribution of the data, affect the mean and standard deviation, and reduce the accuracy of the machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ee5e7",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33853e",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in customer data analysis:\n",
    "\n",
    "1. Deletion: In this technique, the observations or variables with missing data are deleted from the dataset. This method is simple and straightforward but can result in loss of information and reduce the sample size.\n",
    "\n",
    "2. Imputation: In this technique, missing values are replaced with estimated values based on the available data. Imputation can be done using various methods, including mean imputation, median imputation, mode imputation, and regression imputation.\n",
    "\n",
    "3. Multiple imputation: In this technique, missing values are imputed multiple times to generate multiple complete datasets. Each dataset is analyzed separately, and the results are combined to produce final estimates and standard errors. Multiple imputation can provide more accurate results than single imputation methods.\n",
    "\n",
    "4. K-nearest neighbors imputation: In this technique, missing values are imputed by using the values of the k-nearest neighbors in the dataset. This method is based on the assumption that the observations that are similar in other variables will have similar values for the missing variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ca41a",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a4a28",
   "metadata": {},
   "source": [
    "To determine if the missing data is missing at random or if there is a pattern to the missing data, some strategies that can be used include:\n",
    "\n",
    "Visual inspection: One way to determine if there is a pattern to the missing data is to visualize the missing data using a heatmap or a missingness plot. A heatmap will show the missingness pattern for each variable, while a missingness plot will show the overall percentage of missing data in the dataset. If there is a pattern to the missing data, it will be visible in these plots.\n",
    "\n",
    "Statistical tests: Another way to determine if the missing data is missing at random is to use statistical tests. For example, the Little's test or the Missing Completely at Random (MCAR) test can be used to test if the missing data is random or not. These tests compare the observed data with the expected values if the data were missing at random.\n",
    "\n",
    "Imputation: Another approach is to impute the missing data using different methods and evaluate the impact of the imputation method on the analysis. If the imputation method has a significant impact on the results, it may indicate that the missing data is not missing at random.\n",
    "\n",
    "Expert knowledge: Finally, it may be useful to consult with experts in the field to determine if there is a reason for the missing data. For example, if the missing data is related to a specific group of individuals, it may indicate that there is a pattern to the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34a1c1",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b11b5",
   "metadata": {},
   "source": [
    "When dealing with imbalanced datasets in medical diagnosis projects, there are several strategies that can be used to evaluate the performance of a machine learning model:\n",
    "\n",
    "Confusion matrix: A confusion matrix is a table that summarizes the true positive, true negative, false positive, and false negative results of a binary classification model. It can be used to calculate performance metrics such as accuracy, precision, recall, F1 score, and AUC-ROC curve.\n",
    "\n",
    "Resampling: Resampling techniques such as oversampling and undersampling can be used to balance the dataset. Oversampling involves creating synthetic examples of the minority class to increase the number of observations in the dataset, while undersampling involves reducing the number of observations in the majority class. These techniques can help balance the dataset and improve the model's performance.\n",
    "\n",
    "Cost-sensitive learning: In cost-sensitive learning, the misclassification costs of the minority class are given higher weight than those of the majority class. This approach can help improve the performance of the model on the minority class.\n",
    "\n",
    "Ensemble methods: Ensemble methods such as bagging, boosting, and stacking can be used to combine multiple classifiers to improve the performance of the model. These methods can help the model handle the imbalanced nature of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80332479",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b97df",
   "metadata": {},
   "source": [
    "To balance an unbalanced dataset with the majority class, you can use different resampling techniques such as undersampling, oversampling, or a combination of both. In your case, you are interested in down-sampling the majority class to balance the dataset. Here are some methods you can employ to achieve this:\n",
    "\n",
    "1. Random under-sampling: This method involves randomly selecting a subset of the majority class observations to match the size of the minority class. This can be easily achieved in Python using the sample() function from the Pandas library.\n",
    "\n",
    "2. Cluster-based under-sampling: This method involves grouping similar data points and selecting only one representative data point from each group. This can be achieved using clustering techniques such as K-Means clustering or Hierarchical clustering.\n",
    "\n",
    "3. Tomek links under-sampling: This method involves removing data points that are close to the decision boundary between the majority and minority classes. This can be achieved using the TomekLinks class from the imblearn library in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256215a",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2b66d",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset, there are several techniques that you can use to balance the dataset and up-sample the minority class. Some of these techniques include:\n",
    "\n",
    "1. Random Oversampling: This technique involves randomly duplicating examples from the minority class until it is balanced with the majority class.\n",
    "\n",
    "2. Synthetic Minority Over-sampling Technique (SMOTE): This technique creates synthetic examples of the minority class by interpolating between neighboring examples.\n",
    "\n",
    "3. Random Under-sampling: This technique involves randomly removing examples from the majority class until the dataset is balanced.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
